Käytä vain requirements.txt määrittelemiä versioita;
Muuten tulee ihmeellisiä yhteensopivuusongelmia, mm. import gym valittaa jotain cv2:sta. 
Liittyy jotenkin stable baselines 3, koska ongelma tulee vastaan sen asentamisen jälkeen

updt. Uusi stable baselines3 käyttää gymnasiumia ja cleanrl alunperin käyttää gymia. 
Ilmeisesti gym + gymnasium asentaminen hajoittaa import gymin.


#### 21.8 ####
Unity environmentin "get_steps" komento palauttaa envin staten. Jos agentin decision period (määritelty unityssä) on yli 1,
jokaisella training stepillä pythonin puolella ei välttämättä siltä agentilta tule mitään decision pyyntöä.
-> Joissain stepeissä ei tule mitään observaatioita tai vain osalta agenteista tulee observaatio.
Tällä hetkellä käyttämällä pelkästään decision period = 1 koodi toimii.

Tällä hetkellä koodi käyttää rllib menetelmää erotella agentit ("Crawler?team0_0" tms. avaimilla dictionaryssä).
Periaatteessa toimii, mutta todennäköisesti aiheuttaa melkoisia perffiongelmia pythonin puolella.
Lisäksi tällä hetkellä replay-bufferin samplejen tallennus on kyseenalainen. Unity pyörii ~1fps 2. agentilla -> about 1 training step per sekunti?
Pitää testata profilerilla mikä tuossa kestää todellisuudessa, yksi varmaan on torch-numpy-torch muunnokset unity envin takia


#### 28.8 #### 
SAC ei välttämättä ole hyvä vaihtoehto tässä? Kouluttaminen ei tunnu edistyvän, edes ~150k training stepin jälkeen.
Hidas fps johtuu varsinaisesta koulutuksesta; ns. alkuperäinen cleanrl toteutus on myös hidas, koska jokaisella training stepillä 
sampletaan defaulttina 256 aikaisempaa steppiä. CleanRL PPO koodi testiin.


#### 30.8 ####
PPO koulutus toimii huomattavasti nopeammin, unity pyörii mlagents Crawler ympäristössä 10 agentilla ~300fps, eli koulutus ei bottleneckkaa
perffiä merkittävästi. 

Kouluttaminen ei kuitenkaan tunnu edistyvän myöskään PPO:lla. Ilmeisesti ongelma on että on käytössä decision period = 1 joka johtaa
että jokaisella unity framella(? vai physics step?) Unity requestaa stepin. Netissä (esim. https://community.arm.com/arm-community-blogs/b/graphics-gaming-and-vr-blog/posts/3-unity-ml-agents-on-arm-how-we-created-game-ai)
mainitaan että liian pieni decision period voi negatiivisesti vaikuttaa toimintaan. Pitää koittaa saada korjattua toimimaan, niin että
jokainen steppi ei palauta observaatiota ja ei anneta actionia.


#### 4.9 ####
TODO: ppo_continuous_action jatkaa muuttamista multi-agentiksi. Pitää tehdä oma luokka sync_vector_envistä joka tukee multi-agenttia.